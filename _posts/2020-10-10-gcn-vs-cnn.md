---
layout: post
comments: true
categories: Work
title:  "CNN与GCN的区别、联系与融合"
excerpt: "CNN与GCN的区别、联系与融合"
date:   2020-10-10 14:00:00
---

#### self-attention、non local networks与gcn
self-attention、non local networks和gcn都可以用下面的公式（1）来表征[2]

```math
y_i = \frac{1}{C(x)} \sum_{\forall{j}} {f(x_i, x_j)g(x_j)}
```
其中
```math
x_i \in \mathbb{R}^{fin \times 1}，是i这个node的特征
```

```math
f(x_i, x_j) \in \mathbb{R}^0，度量两个node之间的相似度
```

```math
C(x) \in \mathbb{R^{0}}，归一化
```

```math
g(x) \in \mathbb{R^{fout \times 1}},空间变换
```

```math
y_i \in \mathbb{R^{fout \times 1}}
```

```math
fin为输入特征通道数，fout为输出特征通道数
```

##### 比如self attention里面
```math
g(x_j) = W_g x_j
```

```math
f(x_i,x_j) = x_{i}^T W_\theta^T W_\phi x_j 
```
其中
```math
W_g, W_\theta, W_\phi \in \mathbb{R^{fout \times fin}}
```

##### 在non local networks里
```math
f(x_i,x_j) = e^{x_{i}^T W_\theta^T W_\phi x_j}
```

##### 在gcn里
```math
f(x_i,x_j)定义为i和j两个node之间是否有边，1表示有边相连，0表示没有边
```

#### cnn
cnn可以用如下公式（2）表征（为了方便讨论，假设为一维时序数据）

```math
y_i = \sum_{j=0}^{L-1} W_j x_{i+j-\frac{L}{2}}
```
其中
```math
L为卷积核长度
```

```math
W_j \in \mathbb{R^{fout \times fin}}
```

```math
y_i \in \mathbb{R^{fout \times 1}}
```

设
```math
g_j(x_{i+j-\frac{L}{2}}) = W_j x_{i+j-\frac{L}{2}}
```
则由公式（2）可得到公式（3）
```math
y_i = \sum_{j=0}^{L-1} g_j(x_{i+j-\frac{L}{2}})
```

#### 区别与联系
对比公式（1）和公式（3）
```math
y_i = \frac{1}{C(x)} \sum_{\forall{j}} {f(x_i, x_j)g(x_j)}
```

```math
y_i = \sum_{j=0}^{L-1} g_j(x_{i+j-\frac{L}{2}})
```
##### 每个node的空间变换
（1）和（3）都会对node做空间变换即
`$g_j(x_{i+j-\frac{L}{2}})$`
和
`$g(x_j)$`
但是（1）中每个点的空间变换是相同的，而（3）中不同点做的空间变换不同

##### 对node与node之间相关性的建模
（3）中通过`$f(x_i, x_j)$`对每个空间变化之后的特征进行加权，`$f(x_i, x_j)$`（即self-attention）显式地对node与node之间的相关性进行了建模，而且这个相关性相对输入而言是动态的。

（1）中虽然没显式地针对node与node之间地相关性进行建模，但是每个node的空间变换不相同，这个不相同隐式地包含了点与点之间的相关性，不过这个相关性相对输入而言是静态的，即无论输入怎么变，点与点之间的相关性一经训练完成就再也不会变化。

这个有点像bn和senet，bn中的`$\gamma$`对每个通道有一个加权，senet会通过se模块去学每个通道的加权（论文中叫attention），bn中的`$\gamma$`是静态的，训练结束后每个通道的加权不会根据输入的变化而变化，而senet中的attention是动态的，每个通道上的attention会因为输入不同而发生变化。

##### 局部与全局
（1）中`$y_i$`取决于上一层所有的node，而（3）中`$y_i$`仅取决于`$i$`附近的node（卷积核的size决定其范围）


#### 融合
公式（1）和（3）既有相似又有区别，那么很直接的一个想法是能否对它们的特性做排列组合，得到更好的模型呢？

比如，结合（3）中的局部性和（1）中的self-attention。
```math
y_i = \sum_{j=0}^{L-1} {f(x_i, x_j)g(x_j)}
```


再比如，结合（3）中不同点之间不共享空间变化这个特点和（1）中的self-attention。

```math
y_i = \frac{1}{C(x)} \sum_{\forall{j}} {f(x_i, x_j)g_j(x_j)}
```

上述两个直接的排列组合都能在[3]这篇论文里看到应用。

---
Reference

[1]Bello, Irwan, et al. "Attention augmented convolutional networks."Proceedings of the IEEE International Conference on Computer Vision. 2019.

[2]Wang, Xiaolong, et al. "Non-local neural networks."Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.

[3]Ramachandran, Prajit, et al. "Stand-alone self-attention in vision models."arXiv preprint arXiv:1906.05909(2019).
