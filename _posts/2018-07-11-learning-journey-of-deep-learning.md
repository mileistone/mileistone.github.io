---
layout: post
comments: true
title: 深度学习的学习历程
excerpt: 看山是山，看山不是山，看山还是山
categories: Work
date: 2018-07-11 20:00:00
---

刚入门深度学习的时候，我看了各种深度学习相关的资料，花书、cs231n、neural networks and deep learning、cs224d等等。

看来看去，感觉好像什么都懂了，不就那些模块吗，conv、lstm、pooling、fc、drop out等等，这些模块的公式早就能背得滚瓜烂熟。AlexNet、
VGG、GoogLeNet、ResNet等网络就像乐高一样，把这些模块当积木一样组合起来，好像也没啥特别的。

又好像什么都不懂，学会这些模块的公式就算会深度学习了吗？整个深度学习的学习周期是怎样的，我下一步应该干啥？这些模块看起来平平无奇，
为什么组合在一起就能发挥这么大威力？为什么drop out能起到正则作用？L1正则和L2正则有什么区别？cnn、rnn、dnn除了公式不一样外到底有
啥区别？诶，最后一个fc层看起来跟多类别lr分类器好像啊，它们是一回事吗？各种各样的问题，不一而足，而每个问题我都不知道答案。

看的这些资料里面要么没讲到这些问题，要么用比较数学的方式解释，对初学者非常不友好。

我觉得初学者最缺的不是深度学习的资料，以及那一堆公式，而是一个指路人，他能用通俗易懂的方式把深度学习在你面前掰开，又亲手把它给捏回去，
还能告诉你深度学习整个的学习周期是怎样的，这样就能让你少走很多弯路，提高学习的效率。

如果现在可以穿越到我刚入门的时候，我会这样跟当时的自己讲：

深度学习的资料汗牛充栋，不过入门看其中一两本经典的书就够了，比如花书《Deep Learning》和stanford的cs231n。

入门不要贪多，不要贪图一下就啥都能理解。入门一定要快，不要恋战，比较难理解的知识点先跳过去。入门的目的是对深度学习的历史、概貌
有个大致了解，知道深度学习能干什么。

深度学习入门的确容易，就那么几个模块，conv、rnn、relu、pooling、fc等等，只要你懂线性代数、求导，然后看一两本经典的书就入门了。

但是想学好实际上却不那么容易。我觉得学习深度学习分为三个阶段。

###### 一、看山是山

conv、rnn、relu、pooling、fc等等模块的公式背得滚瓜烂熟，定义烂熟于心，但是别人要问两个为什么，立马就招架不住了。

这个阶段主要是看教材、课程，打好基础。

###### 二、看山不是山

conv不就是模板匹配+sliding window嘛，跟用hog进行行人检测的过程多类似呀，只不过conv里的模板参数可以学；drop out不就是集成学习的思想嘛，
它跟random forest多像啊；L1正则和L2正则是加在模型上的prior，比如L1正则假定了一个拉普拉斯分布，L2正则假定了一个高斯分布；fc不就是矩阵
里空间变换嘛；最后一层fc加softmax不就是多分类lr嘛，之前的部分可以看做一个特征提取器，然后用多分类lr对特征进行分类。

CNN和RNN是加了assumption的DNN。CNN的assumption是数据在二维空间上存在着相关性，RNN的assumption是数据在一维空间上存在着相关性。心想“诶，
一维空间是二维空间的特例，那CNN岂不是可以用来解决需要使用rnn的问题？我擦，大发现，看来可以搞一波事情了，谷歌了一下，发现Facebook已经用CNN
来做翻译了，holy shit，晚了一步”。

这个阶段主要是思考上个阶段看的东西，将书本里的知识内化为自己的知识。

###### 三、看山还是山

慢慢意识到，没有最好的模型，只有最合适的模型。之前听到实验室学弟问“深度学习这么厉害，为啥还要学LR、naive bayes、SVM这些low的模型”，
我想这应该是很多初学者的疑问，我当初也有这样的疑问。

尺有所长，寸有所短。每个模型都有它适用的范围（其实也就是assumption），深度学习也不例外，超过了适用范围，啥模型都得嗝屁。比如你的数据天
然是线性可分的，那LR或者SVM将会是最好的选择，如果你选了高大上的深度学习，结果反而会适得其反。

面对一个任务，分析这个任务的assumption，然后去你的武器库（也就是各种模型）里寻找跟这个assumption匹配的武器，知己知彼，方能百战不殆。
不要瞧不起SVM这样的匕首，也不要太高看深度学习这样的屠龙刀。

这个阶段就是要融会贯通，无招胜有招。大音希声，大象无形，武功高强者，飞花摘叶俱可伤人。
